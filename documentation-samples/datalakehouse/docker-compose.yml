version: "3"

services:

  starrocks-fe:
    image: starrocks/fe-ubuntu:3.2-latest
    hostname: starrocks-fe
    user: root
    command: |
      sh /opt/starrocks/fe/bin/start_fe.sh
    ports:
      - 8030:8030
      - 9020:9020
      - 9030:9030
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
    healthcheck:
      test: 'mysql -u root -h starrocks-fe -P 9030 -e "SHOW FRONTENDS\G" |grep "Alive: true"'
      interval: 10s
      timeout: 5s
      retries: 3

  starrocks-be:
    image: starrocks/be-ubuntu:3.2-latest
    command:
      - /bin/bash
      - -c
      - |
        ulimit -u 65535;
        ulimit -n 65535;
        echo "# Enable data cache"  >> /opt/starrocks/be/conf/be.conf
        echo "block_cache_enable = true"  >> /opt/starrocks/be/conf/be.conf
        echo "block_cache_mem_size = 536870912" >> /opt/starrocks/be/conf/be.conf
        echo "block_cache_disk_size = 1073741824" >> /opt/starrocks/be/conf/be.conf
        sleep 15s
        mysql --connect-timeout 2 -h starrocks-fe -P 9030 -u root -e "ALTER SYSTEM ADD BACKEND \"starrocks-be:9050\";"
        /opt/starrocks/be/bin/start_be.sh
    ports:
      - 8040:8040
    hostname: starrocks-be
    user: root
    depends_on:
      - starrocks-fe
    healthcheck:
      test: 'mysql -u root -h starrocks-fe -P 9030 -e "SHOW BACKENDS\G" |grep "Alive: true"'
      interval: 10s
      timeout: 5s
      retries: 3

  metastore_db:
    image: postgres:11
    hostname: metastore_db
    environment:
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      POSTGRES_DB: metastore

  hive-metastore:
    hostname: hive-metastore
    image: 'starburstdata/hive:3.1.2-e.18'
    ports:
      - '9083:9083' # Metastore Thrift
    environment:
      HIVE_METASTORE_DRIVER: org.postgresql.Driver
      HIVE_METASTORE_JDBC_URL: jdbc:postgresql://metastore_db:5432/metastore
      HIVE_METASTORE_USER: hive
      HIVE_METASTORE_PASSWORD: hive
      HIVE_METASTORE_WAREHOUSE_DIR: s3a://huditest/
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: admin
      S3_SECRET_KEY: password
      S3_PATH_STYLE_ACCESS: "true"
      REGION: ""
      GOOGLE_CLOUD_KEY_FILE_PATH: ""
      AZURE_ADL_CLIENT_ID: ""
      AZURE_ADL_CREDENTIAL: ""
      AZURE_ADL_REFRESH_URL: ""
      AZURE_ABFS_STORAGE_ACCOUNT: ""
      AZURE_ABFS_ACCESS_KEY: ""
      AZURE_WASB_STORAGE_ACCOUNT: ""
      AZURE_ABFS_OAUTH: ""
      AZURE_ABFS_OAUTH_TOKEN_PROVIDER: ""
      AZURE_ABFS_OAUTH_CLIENT_ID: ""
      AZURE_ABFS_OAUTH_SECRET: ""
      AZURE_ABFS_OAUTH_ENDPOINT: ""
      AZURE_WASB_ACCESS_KEY: ""
      HIVE_METASTORE_USERS_IN_ADMIN_ROLE: "admin"
    depends_on:
      - metastore_db
    healthcheck:
      test: bash -c "exec 6<> /dev/tcp/localhost/9083"

  minio:
    image: minio/minio
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=password
      - MINIO_DOMAIN=minio
    networks:
      default:
        aliases:
          - warehouse.minio
    ports:
      - 9001:9001
      - 9000:9000
    command: ["server", "/data", "--console-address", ":9001"]
  mc:
    depends_on:
      - minio
    image: minio/mc
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
    entrypoint: >
      /bin/sh -c "
      until (/usr/bin/mc config host add minio http://minio:9000 admin password) do echo '...waiting...' && sleep 1; done;
      /usr/bin/mc rm -r --force minio/warehouse;
      /usr/bin/mc mb minio/warehouse;
      /usr/bin/mc policy set public minio/warehouse;
      tail -f /dev/null
      "

  spark-hudi:
    image: 'ghcr.io/trinodb/testing/spark3-hudi:latest'
    platform: linux/amd64
    hostname: spark-hudi 
    depends_on:
      hive-metastore:
        condition: service_healthy
      minio:
        condition: service_started
    ports:
      - 4041:4041
      - 4042:4042
      - 4043:4043
    volumes:
      - ./spark-conf/spark-defaults.conf:/spark-3.2.1-bin-hadoop3.2/conf/spark-defaults.conf:ro
      - ./spark-conf/core-site.xml:/spark-3.2.1-bin-hadoop3.2/conf/core-site.xml:ro
      - ./spark-conf/hadoop-metrics2-s3a-file-system.properties:/spark-3.2.1-bin-hadoop3.2/conf/hadoop-metrics2-s3a-file-system.properties:ro
      - ./spark-conf/hadoop-metrics2-hbase.properties:/spark-3.2.1-bin-hadoop3.2/conf/hadoop-metrics2-hbase.properties:ro
      - ./spark-conf/hudi-defaults.conf:/etc/hudi/conf/hudi-defaults.conf:ro
      - ./spark-jars:/spark-3.2.1-bin-hadoop3.2/auxjars
    environment:
      MINIO_ACCESS_KEY: admin
      MINIO_SECRET_KEY: password
      HUDI_CONF_DIR: /etc/hudi/conf

  postgresql:
    image: quay.io/debezium/postgres:16
    restart: always
    environment:
      POSTGRES_PASSWORD: postgres
      POSTGRES_USER: postgres
#    volumes:
#      - ./pgdata:/var/lib/postgresql/data
    ports:
      - 5432:5432

  adminer:
    image: adminer
    restart: always
    ports:
      - 8080:8080
    depends_on:
      postgresql:
        condition: service_started

  zookeeper:
    image: quay.io/debezium/zookeeper:2.6
    ports:
      - 2181:2181
      - 2888:2888
      - 3888:3888
    healthcheck:
      test: bash -c "exec 6<> /dev/tcp/localhost/2181"

  kafka:
    image: quay.io/debezium/kafka:2.6
    ports:
      - 9092:9092
      - 29094:29094
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      ZOOKEEPER_CONNECT: zookeeper:2181
# KAFKA LISTENERS ENV allow you to connect from host machine using localhost:9092 while docker compose kafka uses kafka:29092
      KAFKA_LISTENERS: LISTENER_BOB://kafka:29092,LISTENER_FRED://kafka:9092,LISTENER_ALICE://kafka:29094
      KAFKA_ADVERTISED_LISTENERS: LISTENER_BOB://kafka:29092,LISTENER_FRED://localhost:9092,LISTENER_ALICE://never-gonna-give-you-up:29094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_BOB:PLAINTEXT,LISTENER_FRED:PLAINTEXT,LISTENER_ALICE:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_BOB
    healthcheck:
      test: /kafka/bin/kafka-cluster.sh cluster-id --bootstrap-server kafka:29092 || exit 1
      interval: 1s
      timeout: 60s
      retries: 60

  connect:
    image: quay.io/debezium/connect:2.6
    ports: 
      - 8083:8083
      - 5005:5005
    environment:
      - BOOTSTRAP_SERVERS=kafka:29092
      - GROUP_ID=1
      - CONFIG_STORAGE_TOPIC=my_connect_configs
      - OFFSET_STORAGE_TOPIC=my_connect_offsets
      - STATUS_STORAGE_TOPIC=my_source_connect_statuses
      - CONNECT_PLUGIN_PATH=/kafka/connect,/opt/connectors
#      - CONNECT_LOG4J_LOGGERS=TRACE,stdout
#      - LOG_LEVEL=DEBUG
      - JAVA_TOOL_OPTIONS="-agentlib:jdwp=transport=dt_socket,address=*:5005,server=y,suspend=n"
# KAFKA_DEBUG=y doesn't bind to all hostnames so you have to use JAVA_TOOL_OPTIONS
#      - KAFKA_DEBUG=y
#      - DEBUG_SUSPEND_FLAG=y
    depends_on:
      kafka:
        condition: service_healthy
    volumes:
      - ./kafka-connect-connectors-jar:/opt/connectors
      - /Users/atwong/sandbox/debezium-connector-jdbc/target/debezium-connector-jdbc:/kafka/connect/debezium-connector-jdbc
#      ./kafka-conf/log4j.properties:/kafka/config/log4j.properties:ro

  starrocks-toolkit:
    image: atwong/starrocks-tool-box
    hostname: starrocks-toolkit
    volumes:
      - ./srtooldata:/home/data
    entrypoint: > 
      /bin/sh -c "
      mysql -P 9030 -h starrocks-fe -u root -e \"create database demo\";
      mysql -P 9030 -h starrocks-fe -u root -e \"ADMIN SET FRONTEND CONFIG ('default_replication_num' = '1')\";
      tail -f /dev/null;
      "
    depends_on:
      starrocks-fe:
        condition: service_healthy

networks:
  default:
     name: lakehouse
